worker_processes  1;

events { worker_connections 1024; }

http {
  include       mime.types;
  default_type  application/octet-stream;
  sendfile        on;
  keepalive_timeout  65;
  server_tokens off;

  map $http_upgrade $connection_upgrade {
    default upgrade;
    ''      close;
  }

  server {
    listen 80;
    server_name _;

    # Frontend
    location / {
      root   /usr/share/nginx/html;
      index  index.html;
      try_files $uri /index.html;
    }

    # API proxy
    location /api/ {
      proxy_pass http://api:8000/api/;
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_http_version 1.1;
      proxy_set_header Connection "";
    }

    # Optional: expose Kiwix under /kiwix/
    location /kiwix/ {
      proxy_pass http://kiwix:8080/;
      proxy_set_header Host $host;
    }

    # Optional: expose local Ollama OpenAI endpoint (debug only)
    # Requires Ollama running on the host (macOS/Windows Docker Desktop supports host.docker.internal)
    location /v1/ {
      proxy_pass http://host.docker.internal:11434/v1/;
      proxy_set_header Host $host;
    }
  }
}
